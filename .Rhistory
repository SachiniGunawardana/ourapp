y<-c(19.4,32.6,27,32.1,33,17.7,24.8,27.9,25.2,NA,17,19.4,9.1,11.9,NA,20.7,21,20.5,18.8,18.6,14.3,14.4,11.8,11.6,14.2,17.3,19.4,19.1,16.9,20.8)
w<-gl(6,5,labels=c("Diet1","Diet2","Diet3","Diet4","Diet5","Die6t"))
par(mfrow=c(2,2))
plot(aov(y~w))
bartlett.test(y~w)
summary(aov(y~site))
summary(aov(y~w))
pairwise.t.test(y,w,p.adujmenthod="bonferroni")
y<-c(29.21,28.25,28.2,28.62,26.18,26.02,26.22,25.56,30.91,30.18,30.52,30.09,25.14,25.26,25.2,25.02,26.16,25.14,25.26,25.46)
w<-gl(5,4,labels=c("A","B","C","D","E"))
#chech assumptions
#CHECK THE NORMALITY
shapiro.test(y)
#Test of equal variance
bartlett.test(y~w)
summary(aov(y~w))
TukeyHSD(aov(y~w),ordered=F,conf.level=0.99)
hard<-c(9.3,9.4,9.6,10.1,9.4,9.3,9.8,9.9,9.2,9.4,9.5,9.7,9.7,9.6,10.0,10.2)
tip<-gl(4,4)
coupon<-gl(4,1,16)
tip
coupon
summary(aov(hard~tip+coupon))
#l<-lm(hard~tip+coupon)
#anova(l)
par(mfrow=c(2,2))
plot(aov(hard~tip+coupon))
plot(residuals(aov(hard~tip+coupon)))
q()
install.packages("C:/Users/ACER/Downloads/SocialMediaLab_0.18.0.tar.gz", repos = NULL, type = "source")
install.packages("magrittr")
require(magrittr)
# Authenticate with youtube, Collect data from youtube and Create an actor network
Authenticate("youtube", apiKey= "SyAWVpmJgfXe_BL1HaoEuH9L5Z-YluBR1AU") %>% Collect(videoIDs = videoIDs) %>% Create("Actor")
install.packages("SocialMediaMineR")
# Authenticate with youtube, Collect data from youtube and Create an actor network
#Authenticate("youtube", apiKey= "SyAWVpmJgfXe_BL1HaoEuH9L5Z-YluBR1AU") %>% Collect(videoIDs = videoIDs) %>% Create("Actor")
require(SocialMediaLab)
# Authenticate with youtube, Collect data from youtube and Create an actor network
#Authenticate("youtube", apiKey= "SyAWVpmJgfXe_BL1HaoEuH9L5Z-YluBR1AU") %>% Collect(videoIDs = videoIDs) %>% Create("Actor")
require(SocialMediaMineR)
get_linkedin("http://www.nytimes.com/")
install.packages("C:/Users/ACER/Downloads/SocialMediaLab_0.23.2.tar.gz", repos = NULL, type = "source")
install.packages("shiny")
library("shiny", lib.loc="~/R/win-library/3.4")
install.packages("tm")
library("tm", lib.loc="~/R/win-library/3.4")
detach("package:tm", unload=TRUE)
library("tm", lib.loc="~/R/win-library/3.4")
install.packages("worldcloud")
version
shiny::runApp('D:/cloud/project/app')
##########create classifier###########
######################################
#read dataset which is having randomized rows
data<-read.csv("modified_rand - Copy.csv")
data<-data[,c(3,6)] #select comments and ctegory columns
# Cleaning the texts
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(data$comment_text))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
#dataset = as.data.frame(as.matrix(dtm))
#partitioning dataset into training and test sets
dtm_train <- dtm[1:2000, ] #training set
dtm_test <- dtm[2001:2498, ] #test set
train_labels <- data[1:2000,]$category # categories in training set
test_labels <- data[2001:2498, ]$category #categories in test set
prop.table(table(train_labels))
prop.table(table(test_labels))
#creating indicator features for frequent words from dtm_train
#Take a document term matrix and returns a character vector containing the words appearing at least 5 times.
freq_words <- findFreqTerms(dtm_train, 5)
str(freq_words)
#Filter DTM to include only the terms appearing in a specified vector.
dtm_freq_train<- dtm_train[ , freq_words]
dtm_freq_test <- dtm_test[ , freq_words]
# change to a categorical variable that simply indicates yes or no depending on whether the  word appears at all.
convert_counts <- function(x) { x <- ifelse(x > 0, "Yes", "No") }
train <- apply(dtm_freq_train, MARGIN = 2, convert_counts)
test <- apply(dtm_freq_test, MARGIN = 2, convert_counts)
str(train)
str(test)
library(e1071)
#train a model
classifier <- naiveBayes(train, train_labels)
#Evaluate model performance
test_pred <- predict(classifier, test)
#library(gmodels)
## Warning: package 'gmodels' was built under R version 3.2.5
library("caret")
#CrossTable(test_pred, test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
confusionMatrix(table(test_pred,test_labels))
conf.mat<-table(test_pred,test_labels)
accuracy <- sum(diag(conf.mat)) / 498 * 100
#read dataset which is having randomized rows
data<-read.csv("modified_rand - Copy.csv")
data<-data[,c(3,6)] #select comments and ctegory columns
# Cleaning the texts
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(data$comment_text))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
#dataset = as.data.frame(as.matrix(dtm))
#partitioning dataset into training and test sets
dtm_train <- dtm[1:2000, ] #training set
dtm_test <- dtm[2001:2498, ] #test set
train_labels <- data[1:2000,]$category # categories in training set
test_labels <- data[2001:2498, ]$category #categories in test set
prop.table(table(train_labels))
prop.table(table(test_labels))
#creating indicator features for frequent words from dtm_train
#Take a document term matrix and returns a character vector containing the words appearing at least 5 times.
freq_words <- findFreqTerms(dtm_train, 5)
str(freq_words)
#Filter DTM to include only the terms appearing in a specified vector.
dtm_freq_train<- dtm_train[ , freq_words]
dtm_freq_test <- dtm_test[ , freq_words]
# change to a categorical variable that simply indicates yes or no depending on whether the  word appears at all.
convert_counts <- function(x) { x <- ifelse(x > 0, "Yes", "No") }
train <- apply(dtm_freq_train, MARGIN = 2, convert_counts)
test <- apply(dtm_freq_test, MARGIN = 2, convert_counts)
str(train)
str(test)
library(e1071)
#train a model
classifier <- naiveBayes(train, train_labels)
#Evaluate model performance
test_pred <- predict(classifier, test)
#library(gmodels)
## Warning: package 'gmodels' was built under R version 3.2.5
library("caret")
#CrossTable(test_pred, test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
confusionMatrix(table(test_pred,test_labels))
#read dataset which is having randomized rows
data<-read.csv("modified_rand - Copy.csv")
data<-data[,c(3,6)] #select comments and ctegory columns
# Cleaning the texts
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(data$comment_text))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
#dataset = as.data.frame(as.matrix(dtm))
#partitioning dataset into training and test sets
dtm_train <- dtm[1:2000, ] #training set
dtm_test <- dtm[2001:2498, ] #test set
train_labels <- data[1:2000,]$category # categories in training set
test_labels <- data[2001:2498, ]$category #categories in test set
prop.table(table(train_labels))
prop.table(table(test_labels))
#creating indicator features for frequent words from dtm_train
#Take a document term matrix and returns a character vector containing the words appearing at least 5 times.
freq_words <- findFreqTerms(dtm_train, 5)
str(freq_words)
#Filter DTM to include only the terms appearing in a specified vector.
dtm_freq_train<- dtm_train[ , freq_words]
dtm_freq_test <- dtm_test[ , freq_words]
# change to a categorical variable that simply indicates yes or no depending on whether the  word appears at all.
convert_counts <- function(x) { x <- ifelse(x > 0, "Yes", "No") }
train <- apply(dtm_freq_train, MARGIN = 2, convert_counts)
test <- apply(dtm_freq_test, MARGIN = 2, convert_counts)
str(train)
str(test)
library(e1071)
#train a model
classifier <- naiveBayes(train, train_labels)
#Evaluate model performance
test_pred <- predict(classifier, test)
#library(gmodels)
## Warning: package 'gmodels' was built under R version 3.2.5
library("caret")
#CrossTable(test_pred, test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
confusionMatrix(table(test_pred,test_labels))
#read dataset which is having randomized rows
data<-read.csv("modified_rand - Copy.csv")
setwd("D:/cloud/project")
#read dataset which is having randomized rows
data<-read.csv("modified_rand - Copy.csv")
data<-data[,c(3,6)] #select comments and ctegory columns
# Cleaning the texts
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(data$comment_text))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
# Creating the Bag of Words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
#dataset = as.data.frame(as.matrix(dtm))
#partitioning dataset into training and test sets
dtm_train <- dtm[1:2000, ] #training set
dtm_test <- dtm[2001:2498, ] #test set
train_labels <- data[1:2000,]$category # categories in training set
test_labels <- data[2001:2498, ]$category #categories in test set
prop.table(table(train_labels))
prop.table(table(test_labels))
#creating indicator features for frequent words from dtm_train
#Take a document term matrix and returns a character vector containing the words appearing at least 5 times.
freq_words <- findFreqTerms(dtm_train, 5)
str(freq_words)
#Filter DTM to include only the terms appearing in a specified vector.
dtm_freq_train<- dtm_train[ , freq_words]
dtm_freq_test <- dtm_test[ , freq_words]
# change to a categorical variable that simply indicates yes or no depending on whether the  word appears at all.
convert_counts <- function(x) { x <- ifelse(x > 0, "Yes", "No") }
train <- apply(dtm_freq_train, MARGIN = 2, convert_counts)
test <- apply(dtm_freq_test, MARGIN = 2, convert_counts)
str(train)
str(test)
library(e1071)
#train a model
classifier <- naiveBayes(train, train_labels)
#Evaluate model performance
test_pred <- predict(classifier, test)
#library(gmodels)
## Warning: package 'gmodels' was built under R version 3.2.5
library("caret")
#CrossTable(test_pred, test_labels, prop.chisq = FALSE, prop.t = FALSE, dnn = c('predicted', 'actual'))
confusionMatrix(table(test_pred,test_labels))
setwd("D:/cloud/project/ourapp")
setwd("D:/cloud/project/ourapp")
library("rsconnect", lib.loc="~/R/win-library/3.5")
deployApp()
deployApp()
shiny::runApp()
deployApp()
